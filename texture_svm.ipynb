{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texture Synthesis Preparation: Classification with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Application and analysis of texture classification using SVM against dataset of differing textures, in order to prepare for Neural Network based Texture Synthesis, Procedural Texture generation, and applications of Computer Vision object recognition.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Texture Synthesis Requires Pre-Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What is Texture Synthesis?</h2>\n",
    "<h4><b><i>Texture Synthesis::</i></b>\n",
    "<br>\n",
    "Texture Synthesis is the process of generating novel textures from samples of existing textures and imagery, while retaining the visual elements of the original. \n",
    "Texture Synthesis can be used to in 2D and 3D image generation applications, such as::</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Extend existing images and textures mathematically such as procedural textures to solve 2D image tiling</h5>\n",
    "<img src =\"img/TextureTiling.png\" alt=\"TextureTiling.png\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "<i>Blender Guru, \"Blender Texturing for Beginners - Tutorial\"<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Move materials from one type of material to another in existing images in Texture Transfer</h5>\n",
    "<img src =\"img/TextureTransfer.png\" alt=\"TextureTransfer.png\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "<i>University of Illinois, \"Textureshop: Texture Synthesis as as Photograph Editing Tool\"<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Shift portions or entirety of textures from one art style to another via Quilting or Image Analogies and Artistic Filters.</h5>\n",
    "<img src =\"img/TextureAImageAnalogy.png\" alt=\"TextureAnalogy.png\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "<i>University of Washington, \"Texture Synthesis\"<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>And many more applications!</b></h3>\n",
    "<h4>Textures generated via synthesis methods through these approaches are known as Synthetic Textures, and are a precursory method to creating procedural textures in large scale 3D applications.</h4> \n",
    "\n",
    "<h2><b><i>Summary of Goal:: </b></i>Explore the preparation of training classification models on existing textures, in preparation for later Neural Network Synthetic Texture Genesis.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Texture Collection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Origin of the Textures Dataset::</i></b>\n",
    "<br>\n",
    "This data all came from hand, from me walking about my apartment complex taking photos like a total weirdo - we'll call this scientific process <b>Photogrammetric Loitering</b></h4>\n",
    "<li>Size of the dataset can be represented as varying classes of textures depending on how many models and which kinds are used</li>\n",
    "<li>Dataset comprises 5 directories with 8 images for each type, and one 'other' folder for later testing - it is a mixture of other texture types</li>\n",
    "<li>Data cleaning in this project began in this controlled case by my own picking and deleting unwanted or unused textures by hand prior to training</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Pull the Dataset from .Zip Folder 'Textures'::</i></b></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images, extract_path\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Load textures for each category and save their temp directory\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m brick_textures, brick_extract_path \u001b[38;5;241m=\u001b[39m \u001b[43mload_textures_from_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg/textures/brick.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m grass_textures, grass_extract_path \u001b[38;5;241m=\u001b[39m load_textures_from_zip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg/textures/grasses.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m leaves_textures, leaves_extract_path \u001b[38;5;241m=\u001b[39m load_textures_from_zip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg/textures/leaves.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m, in \u001b[0;36mload_textures_from_zip\u001b[1;34m(zip_path)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     23\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file_name)\n\u001b[1;32m---> 24\u001b[0m             img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(file_path)\n\u001b[0;32m     25\u001b[0m             images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images, extract_path\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing the packages\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Function for extracting our zip folders - saving temp directory 'extract_path' to delete later\n",
    "def load_textures_from_zip(zip_path):\n",
    "    images = []\n",
    "    extract_path = zip_path.replace('.zip', '')  # Create a folder path for extraction\n",
    "    with zipfile.ZipFile(zip_path, 'r') as archive:\n",
    "        archive.extractall(extract_path)\n",
    "    for root, _, files in os.walk(extract_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                img = Image.open(file_path)\n",
    "                images.append(img)\n",
    "    return images, extract_path\n",
    "\n",
    "# Load textures for each category and save their temp directory\n",
    "brick_textures, brick_extract_path = load_textures_from_zip(\"img/textures/brick.zip\")\n",
    "grass_textures, grass_extract_path = load_textures_from_zip(\"img/textures/grasses.zip\")\n",
    "leaves_textures, leaves_extract_path = load_textures_from_zip(\"img/textures/leaves.zip\")\n",
    "road_textures, road_extract_path = load_textures_from_zip(\"img/textures/road.zip\")\n",
    "sidewalk_textures, sidewalk_extract_path = load_textures_from_zip(\"img/textures/sidewalk.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data For Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Perform Data Preprocessing - Grayscale Conversion::</i></b></li>\n",
    "<br>\n",
    "In order to classify these textures, they will be first converted into grayscale counterparts.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert textures to grayscale in preparation for Local Binary extraction\n",
    "def convert_to_grayscale(textures):\n",
    "    grayscale_textures = [np.array(texture.convert('L')) for texture in textures]\n",
    "    return np.array(grayscale_textures)\n",
    "\n",
    "# Convert to grayscale\n",
    "brick_textures = convert_to_grayscale(brick_textures)\n",
    "grass_textures = convert_to_grayscale(grass_textures)\n",
    "leaves_textures = convert_to_grayscale(leaves_textures)\n",
    "road_textures = convert_to_grayscale(road_textures)\n",
    "sidewalk_textures = convert_to_grayscale(sidewalk_textures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Perform Data Preprocessing - Local Binary Extraction::</i></b></li>\n",
    "<br>\n",
    "To operate on the now grayscale textures, they will be stored as Local Binary Data in order to compare with the model.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from grayscale textures\n",
    "def extract_lbp_features(textures, radius=3, n_points=8*3):\n",
    "    lbp_features = [local_binary_pattern(texture, P=n_points, R=radius, method='uniform').flatten() for texture in textures]\n",
    "    return np.array(lbp_features)\n",
    "\n",
    "# Extract our features\n",
    "brick_features = extract_lbp_features(brick_textures)\n",
    "grass_features = extract_lbp_features(grass_textures)\n",
    "leaves_features = extract_lbp_features(leaves_textures)\n",
    "road_features = extract_lbp_features(road_textures)\n",
    "sidewalk_features = extract_lbp_features(sidewalk_textures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Perform Data Preprocessing - Cleanup::</i></b></li>\n",
    "<br>\n",
    "Delete the temporarily created directories from .zip extraction in earlier stage.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the extracted folders now that we have our image data\n",
    "shutil.rmtree(brick_extract_path)\n",
    "shutil.rmtree(grass_extract_path)\n",
    "shutil.rmtree(leaves_extract_path)\n",
    "shutil.rmtree(road_extract_path)\n",
    "shutil.rmtree(sidewalk_extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Perform Data Exploration and Analysis::</i></b></li>\n",
    "<br>\n",
    "Build our features and labels, visualizing them prior to model training.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for brick, 1 for grass, 2 for leaves, 3 for road, 4 for sidewalk\n",
    "X = np.vstack([brick_features, grass_features, leaves_features, road_features, sidewalk_features])\n",
    "y = np.array([1]*len(grass_features) + [0]*len(brick_features) + [2]*len(leaves_features) + [3]*len(road_features) + [4]*len(sidewalk_features))\n",
    "\n",
    "# This data is used for grass vs. grass with leaves\n",
    "X_grasses = np.vstack([grass_features, leaves_features])\n",
    "y_grasses = np.array([1]*len(leaves_features) + [0]*len(grass_features))\n",
    "\n",
    "# This data is used for sidewalk vs. road, the most likely to make errors\n",
    "X_stones = np.vstack([road_features, sidewalk_features])\n",
    "y_stones = np.array([1]*len(sidewalk_features) + [0]*len(road_features))\n",
    "\n",
    "# View distribution of the full data as part of data exploration prior to any scaling\n",
    "print(\"Shape of 'X' before Pre-process:\", X.shape)\n",
    "print(\"Shape of 'y' before preprocess:\", y.shape)\n",
    "print(\"\\nClass distribution in 'y' before preprocess:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist = dict(zip(unique, counts))\n",
    "print(class_dist)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=y)\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4], labels=['Brick', 'Grass', 'Leaves', 'Road', 'Sidewalk'])\n",
    "plt.title('Class Distribution Before Scaling')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Perform Data Scaling via Standard Scaler::</i></b></li>\n",
    "<br>\n",
    "Utilize the standard scaler as there are different colors and intensities across images that will need to be scaled.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale our 'X's for various models to train\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_grasses_scaled = scaler.fit_transform(X_grasses)\n",
    "X_stones_scaled = scaler.fit_transform(X_stones)\n",
    "\n",
    "# View data after scaling\n",
    "print(\"Shape of 'X' after scaling:\", X_scaled.shape)\n",
    "print(\"Mean of 'X' after scaling:\", np.mean(X_scaled))\n",
    "print(\"Standard deviation of 'X'\", np.std(X_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Spit Train and Test Sets::</i></b></li>\n",
    "<br>\n",
    "As we are classifying these textures, we will select the SVM model. The target models to analyze and research will be:</h4>\n",
    "<li>Full Texture Set</li>\n",
    "<li>Grass vs. Grass with Leaves</li>\n",
    "<li>Road vs. Sidewalk</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into multiple batches of training data for the varied models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Xg for grasses\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_grasses_scaled, y_grasses, test_size=0.3, random_state=42)\n",
    "\n",
    "# Xst for stones\n",
    "Xst_train, Xst_test, yst_train, yst_test = train_test_split(X_stones_scaled, y_stones, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Fit Models and Store Predictions::</i></b></li>\n",
    "<br>\n",
    "Train models on the SKLearn's SVC model in order to train and predict classification on our various texture datas. We have opted for 'linear' kernel as it is recommended from research done on our binary image data. RBF will also be compared.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train first model type\n",
    "texture_classifier = SVC(kernel='linear')\n",
    "texture_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prediction Storing\n",
    "y_pred = texture_classifier.predict(X_test)\n",
    "\n",
    "# Accuracy Storing\n",
    "accuracy_scores = {}\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy_scores['Main Model [Linear]'] = accuracy\n",
    "print(f\"Overall classification accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "# Train the RBF model type on same data\n",
    "texture_rbf = SVC(kernel='rbf')\n",
    "texture_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = texture_rbf.predict(X_test)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "accuracy_scores['Main Model [RBF]'] = accuracy_rbf\n",
    "\n",
    "# Train and predict on the grass data\n",
    "texture_classifier.fit(Xg_train, yg_train)\n",
    "y_pred_grass = texture_classifier.predict(Xg_test)\n",
    "accuracy_grass = accuracy_score(yg_test, y_pred_grass)\n",
    "accuracy_scores['Grasses [Linear]'] = accuracy_grass\n",
    "\n",
    "texture_rbf.fit(Xg_train, yg_train)\n",
    "y_pred_grass_rbf = texture_rbf.predict(Xg_test)\n",
    "accuracy_grass_rbf = accuracy_score(yg_test, y_pred_grass_rbf)\n",
    "accuracy_scores['Grasses [RBF]'] = accuracy_grass_rbf\n",
    "\n",
    "# Train and predict on the stone data\n",
    "texture_classifier.fit(Xst_train, yst_train)\n",
    "y_pred_stone = texture_classifier.predict(Xst_test)\n",
    "accuracy_stone = accuracy_score(yst_test, y_pred_stone)\n",
    "accuracy_scores['Stones [Linear]'] = accuracy_stone\n",
    "\n",
    "texture_rbf.fit(Xst_train, yst_train)\n",
    "y_pred_stone_rbf = texture_rbf.predict(Xst_test)\n",
    "accuracy_stone_rbf = accuracy_score(yst_test, y_pred_stone_rbf)\n",
    "accuracy_scores['Stones [RBF]'] = accuracy_stone_rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Generate and Display Confusion Matrix, Display Predictions::</i></b></li>\n",
    "<br>\n",
    "Utilizing a heatmap and confusion matrix first, we display the performance of the main target model. We will then evaluate which images were selected for each class.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate main confusion matrix for all texture model\n",
    "texture_cm = confusion_matrix(y_test, y_pred)\n",
    "class_names = ['Brick', 'Grass', 'Leaves', 'Road', 'Sidewalk']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(texture_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b><i>Extensions to the Project::</i></b></li>\n",
    "<br>\n",
    "There are many applications for Texture Synthesis available now that we have successfully classified, to an extent, the textures that we might use. Maybe we want to go to procedural generation immediately and work on generating the borders around existing image textures? Maybe we want to create a specific art style for in-house graphics trained exclusively on synthetic textures and real work from local artists? Many of the next steps would need to take this classifier as a first step in order to train further Deep Learning models - something I will anxiously look forward to in the coming months.</h4>\n",
    "<h4><b><i>A Small List of Potential Applications</i></b></h4>\n",
    "<li>Virtual Textures and Virtual Geometry (Unreal Engine Nanite)</li>\n",
    "<li>Synthetic Normal and Parallax Maps</li>\n",
    "<li><a href=\"https://ieeexplore.ieee.org/document/8942651\">Procedural Texture Generation (Tiny Glade)</a></li>\n",
    "<li><a href=\"https://www.a23d.co/texture/syntheticmaterial-0400882\">Synthetic Materials</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><i>Sources::<i></h4>\n",
    "<li><a href=\"https://courses.cs.washington.edu/courses/cse455/08wi/lectures/texture.pdf\">University of Washington Texture Lecture</a></li>\n",
    "<li><a href=\"https://graphics.cs.yale.edu/sites/default/files/edge-based_procedural_textures.pdf\">Edge-Based Procedural Textures</a></li>\n",
    "<li><a href=\"https://github.com/anopara\">Anastasia Opara GitHub Research Repo</a></li>\n",
    "<li><a href=\"https://medium.com/@allenaltiner/computer-generated-procedural-3d-and-multi-example-texture-based-synthesis-by-the-super-talented-4d1cfe34b05c\">Texture Synthesis from Example</a></li>\n",
    "<li><a href=\"https://pixel.ecn.purdue.edu:8443/projects/ITRweb/content/publication_related/uiuc/textureshop.pdf\">University of Illinois - Textureshop</a></li>\n",
    "<li><a href=\"https://apmonitor.com/pds/index.php/Main/TextureClassification\">Machine Learning for Engineers - Prof. John D. Hedengren, BYU</a></li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
